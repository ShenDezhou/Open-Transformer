{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Mbatc0PQx7j",
        "outputId": "3a749c49-46f2-4d40-b6d1-0eba3f734749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "Cloning into 'Open-Transformer'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 20 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), 446.50 KiB | 1.96 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive\n",
        "!git clone https://github.com/ShenDezhou/Open-Transformer.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Open-Transformer\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hQQd6sPaRUe7",
        "outputId": "138b72fa-168c-4b1d-f79f-7bc79d584b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Open-Transformer\n",
            "Collecting accelerate (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=1.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92 (from -r requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from -r requirements.txt (line 4))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting neptune (from -r requirements.txt (line 5))\n",
            "  Downloading neptune-1.3.2-py3-none-any.whl (455 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.3/455.3 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdbpp (from -r requirements.txt (line 6))\n",
            "  Downloading pdbpp-0.10.3-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (6.4.8)\n",
            "Requirement already satisfied: protobuf==3.20.* in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (6.0)\n",
            "Collecting pynvml (from -r requirements.txt (line 10))\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core (from -r requirements.txt (line 11))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate (from -r requirements.txt (line 12))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (3.8.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.4.0)\n",
            "Collecting rouge_score (from -r requirements.txt (line 15))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.65.0)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.16.3-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 4))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 4))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=2.0.8 (from neptune->-r requirements.txt (line 5))\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune->-r requirements.txt (line 5)) (8.4.0)\n",
            "Collecting PyJWT (from neptune->-r requirements.txt (line 5))\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting backoff (from neptune->-r requirements.txt (line 5))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting boto3>=1.16.0 (from neptune->-r requirements.txt (line 5))\n",
            "  Downloading boto3-1.28.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado<12.0.0,>=11.0.0 (from neptune->-r requirements.txt (line 5))\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune->-r requirements.txt (line 5)) (8.1.3)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune->-r requirements.txt (line 5)) (0.18.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune->-r requirements.txt (line 5)) (1.16.0)\n",
            "Collecting swagger-spec-validator>=2.7.4 (from neptune->-r requirements.txt (line 5))\n",
            "  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune->-r requirements.txt (line 5)) (1.26.16)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune->-r requirements.txt (line 5)) (1.6.0)\n",
            "Collecting fancycompleter>=0.8 (from pdbpp->-r requirements.txt (line 6))\n",
            "  Downloading fancycompleter-0.9.1-py3-none-any.whl (9.7 kB)\n",
            "Collecting wmctrl (from pdbpp->-r requirements.txt (line 6))\n",
            "  Downloading wmctrl-0.4.tar.gz (5.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from pdbpp->-r requirements.txt (line 6)) (2.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (3.1.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (6.3.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (21.3.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (5.3.1)\n",
            "Requirement already satisfied: jupyter-client>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (6.1.12)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (5.9.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (1.5.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (5.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 7)) (0.17.0)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core->-r requirements.txt (line 11))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core->-r requirements.txt (line 11))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting responses<0.19 (from evaluate->-r requirements.txt (line 12))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 13)) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 16)) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 16)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 16)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r requirements.txt (line 16)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->-r requirements.txt (line 16)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->-r requirements.txt (line 16)) (16.0.6)\n",
            "Collecting botocore<1.32.0,>=1.31.0 (from boto3>=1.16.0->neptune->-r requirements.txt (line 5))\n",
            "  Downloading botocore-1.31.0-py3-none-any.whl (11.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.16.0->neptune->-r requirements.txt (line 5))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3>=1.16.0->neptune->-r requirements.txt (line 5))\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune->-r requirements.txt (line 5))\n",
            "  Downloading bravado_core-5.17.1-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune->-r requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune->-r requirements.txt (line 5)) (2.8.2)\n",
            "Collecting simplejson (from bravado<12.0.0,>=11.0.0->neptune->-r requirements.txt (line 5))\n",
            "  Downloading simplejson-3.19.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting monotonic (from bravado<12.0.0,>=11.0.0->neptune->-r requirements.txt (line 5))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting pyrepl>=0.8.2 (from fancycompleter>=0.8->pdbpp->-r requirements.txt (line 6))\n",
            "  Downloading pyrepl-0.9.0.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=2.0.8->neptune->-r requirements.txt (line 5))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook->-r requirements.txt (line 7)) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5)) (4.3.3)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->-r requirements.txt (line 7)) (21.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->notebook->-r requirements.txt (line 7)) (7.34.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->notebook->-r requirements.txt (line 7)) (2.1.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook->-r requirements.txt (line 7)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->-r requirements.txt (line 7)) (2.17.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 16)) (1.3.0)\n",
            "Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->-r requirements.txt (line 5))\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune->-r requirements.txt (line 5))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7)) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7))\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7)) (3.0.38)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5)) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->-r requirements.txt (line 7)) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->notebook->-r requirements.txt (line 7)) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->notebook->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->-r requirements.txt (line 7)) (2.21)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7)) (0.8.3)\n",
            "Collecting fqdn (from jsonschema->swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema->swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema->swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting rfc3339-validator (from jsonschema->swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3987 (from jsonschema->swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5))\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting uri-template (from jsonschema->swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5)) (1.13)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook->-r requirements.txt (line 7)) (0.2.6)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune->-r requirements.txt (line 5))\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, rouge_score, wmctrl, pyrepl\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=5ff368ea9b4fcb8d583dd3713731d553c26fbfddb291c9d4b001872273d7f14d\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c23a7f303b09662fc35ce56d15eb4cbf32792f12c8928bb466d7bc44be90edfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for wmctrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wmctrl: filename=wmctrl-0.4-py3-none-any.whl size=3839 sha256=9c31b5f0e5704fc23ca3d9519801d5e999c33d3c884e9e1e53b8901a040d438b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/fc/49/e978b317217e03bb241c860682bff16775ee7e298eb00230c7\n",
            "  Building wheel for pyrepl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrepl: filename=pyrepl-0.9.0-py3-none-any.whl size=59907 sha256=a745bca9f9ad21903194a76f82467c772a465aad42798cf0dcb1f9cb217b0c48\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/8f/52/5191cdf3a2599696725a5245439fe59d4af6de3bc34edba47e\n",
            "Successfully built antlr4-python3-runtime rouge_score wmctrl pyrepl\n",
            "Installing collected packages: wmctrl, tokenizers, sentencepiece, safetensors, rfc3987, pyrepl, monotonic, antlr4-python3-runtime, xxhash, uri-template, smmap, simplejson, rfc3339-validator, pynvml, PyJWT, omegaconf, jsonref, jsonpointer, jmespath, jedi, fqdn, fancycompleter, dill, backoff, swagger-spec-validator, rouge_score, responses, pdbpp, multiprocess, hydra-core, huggingface-hub, gitdb, botocore, arrow, transformers, s3transfer, isoduration, GitPython, datasets, boto3, evaluate, bravado-core, bravado, neptune, accelerate\n",
            "Successfully installed GitPython-3.1.31 PyJWT-2.7.0 accelerate-0.20.3 antlr4-python3-runtime-4.9.3 arrow-1.2.3 backoff-2.2.1 boto3-1.28.0 botocore-1.31.0 bravado-11.0.3 bravado-core-5.17.1 datasets-2.13.1 dill-0.3.6 evaluate-0.4.0 fancycompleter-0.9.1 fqdn-1.5.1 gitdb-4.0.10 huggingface-hub-0.16.3 hydra-core-1.3.2 isoduration-20.11.0 jedi-0.18.2 jmespath-1.0.1 jsonpointer-2.4 jsonref-1.1.0 monotonic-1.6 multiprocess-0.70.14 neptune-1.3.2 omegaconf-2.3.0 pdbpp-0.10.3 pynvml-11.5.0 pyrepl-0.9.0 responses-0.18.0 rfc3339-validator-0.1.4 rfc3987-1.3.8 rouge_score-0.1.2 s3transfer-0.6.1 safetensors-0.3.1 sentencepiece-0.1.99 simplejson-3.19.1 smmap-5.0.0 swagger-spec-validator-3.0.3 tokenizers-0.13.3 transformers-4.30.2 uri-template-1.3.0 wmctrl-0.4 xxhash-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pdb",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Open-Transformer\n",
        "!python main.py optim.name=adamwscale optim.lr_scheduler=legacy model.compile=true optim.batch_size=16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOBrlEj2Re2N",
        "outputId": "25890b1d-7a50-4b45-b7df-fbff8abb1377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Open-Transformer\n",
            "2023-07-06 14:13:40.117530: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-07-06 14:13:44,382][Main][INFO] - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "[2023-07-06 14:13:44,383][Main][INFO] - Working directory is /content/drive/MyDrive/Open-Transformer/logs/2023-07-06/14-13-44-\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[2023-07-06 14:13:49,065][Main][INFO] - You are using T5 legacy LR Schedule, it's independent from the optim.base_lr\n",
            "[2023-07-06 14:13:49,234][datasets.builder][WARNING] - Found cached dataset text (/root/.cache/huggingface/datasets/text/default-88dde99a484714ba/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "100% 2/2 [00:00<00:00, 856.77it/s]\n",
            "[2023-07-06 14:13:49,244][datasets.arrow_dataset][WARNING] - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-88dde99a484714ba/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-88ff4ea24ebfaf0a.arrow\n",
            "[2023-07-06 14:13:49,245][datasets.arrow_dataset][WARNING] - Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-88dde99a484714ba/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-ad000c73657e10b7.arrow\n",
            "[2023-07-06 14:13:49,253][datasets.arrow_dataset][WARNING] - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-88dde99a484714ba/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b841c62a1efd122a.arrow\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[2023-07-06 14:13:52,780] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-06 14:13:53,257] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __call__\n",
            "[2023-07-06 14:13:53,265] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-06 14:13:53,283] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-06 14:13:53,289] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:13:55,645] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0\n",
            "[2023-07-06 14:13:55,831] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0\n",
            "[2023-07-06 14:13:55,832] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:13:55,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
            "[2023-07-06 14:13:58,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
            "[2023-07-06 14:13:58,500] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:14:08,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1\n",
            "[2023-07-06 14:14:15,943] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1\n",
            "[2023-07-06 14:14:15,945] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:14:18,749] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
            "[2023-07-06 14:14:18,779] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:14:18,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2\n",
            "[2023-07-06 14:14:19,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2\n",
            "[2023-07-06 14:14:19,032] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:14:19,047] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-06 14:14:19,059] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:14:19,102] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3\n",
            "[2023-07-06 14:14:19,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3\n",
            "[2023-07-06 14:14:19,244] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:14:19,254] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
            "[2023-07-06 14:14:23,639] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
            "[2023-07-06 14:14:23,723] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:14:39,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4\n",
            "[2023-07-06 14:14:48,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4\n",
            "[2023-07-06 14:14:48,292] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:14:52,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
            "[2023-07-06 14:14:52,600] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
            "[2023-07-06 14:14:52,603] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:14:52,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5\n",
            "[2023-07-06 14:14:53,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5\n",
            "[2023-07-06 14:14:53,699] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:14:53,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _is_fp16_bf16_tensor\n",
            "[2023-07-06 14:14:53,724] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _is_fp16_bf16_tensor>\n",
            "[2023-07-06 14:14:53,744] torch._dynamo.convert_frame: [INFO] converting frame raised unsupported, leaving it unconverted\n",
            "[2023-07-06 14:14:53,749] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _is_fp16_bf16_tensor>\n",
            "[2023-07-06 14:14:53,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _is_fp16_bf16_tensor\n",
            "[2023-07-06 14:14:53,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _is_fp16_bf16_tensor>\n",
            "[2023-07-06 14:14:53,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _convert_to_fp32\n",
            "[2023-07-06 14:14:53,767] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _convert_to_fp32 (RETURN_VALUE)\n",
            "[2023-07-06 14:14:53,768] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:14:53,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6\n",
            "[2023-07-06 14:14:53,918] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6\n",
            "[2023-07-06 14:14:53,918] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:14:53,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _is_fp16_bf16_tensor\n",
            "[2023-07-06 14:14:53,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _is_fp16_bf16_tensor\n",
            "[2023-07-06 14:14:53,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing is_tensor\n",
            "[2023-07-06 14:14:53,947] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing is_torch_fx_proxy\n",
            "[2023-07-06 14:14:53,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5\n",
            "[2023-07-06 14:14:54,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5\n",
            "[2023-07-06 14:14:54,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4\n",
            "[2023-07-06 14:15:06,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4\n",
            "[2023-07-06 14:15:07,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3\n",
            "[2023-07-06 14:15:08,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3\n",
            "[2023-07-06 14:15:08,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1\n",
            "[2023-07-06 14:15:15,868] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1\n",
            "[2023-07-06 14:15:16,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0\n",
            "[2023-07-06 14:15:16,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0\n",
            "[2023-07-06 14:15:44,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-06 14:15:44,533] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __call__\n",
            "[2023-07-06 14:15:44,541] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-06 14:15:44,557] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-06 14:15:44,566] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:15:44,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7\n",
            "[2023-07-06 14:15:44,774] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7\n",
            "[2023-07-06 14:15:44,775] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:15:44,783] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
            "[2023-07-06 14:15:47,877] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
            "[2023-07-06 14:15:47,921] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:15:56,811] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8\n",
            "[2023-07-06 14:16:05,823] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8\n",
            "[2023-07-06 14:16:05,824] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:16:07,929] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
            "[2023-07-06 14:16:07,948] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:16:07,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9\n",
            "[2023-07-06 14:16:08,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9\n",
            "[2023-07-06 14:16:08,111] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:16:08,118] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-06 14:16:08,124] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:16:08,157] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10\n",
            "[2023-07-06 14:16:08,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10\n",
            "[2023-07-06 14:16:08,261] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:16:08,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
            "[2023-07-06 14:16:12,038] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
            "[2023-07-06 14:16:12,113] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:16:28,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11\n",
            "[2023-07-06 14:16:39,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11\n",
            "[2023-07-06 14:16:39,491] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:16:45,888] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
            "[2023-07-06 14:16:45,913] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
            "[2023-07-06 14:16:45,915] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:16:46,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12\n",
            "[2023-07-06 14:16:47,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12\n",
            "[2023-07-06 14:16:47,249] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:16:47,259] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _is_fp16_bf16_tensor\n",
            "[2023-07-06 14:16:47,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _convert_to_fp32\n",
            "[2023-07-06 14:16:47,265] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing _convert_to_fp32 (RETURN_VALUE)\n",
            "[2023-07-06 14:16:47,266] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-06 14:16:47,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13\n",
            "[2023-07-06 14:16:47,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13\n",
            "[2023-07-06 14:16:47,377] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-06 14:16:47,381] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _is_fp16_bf16_tensor\n",
            "[2023-07-06 14:16:47,384] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _is_fp16_bf16_tensor\n",
            "[2023-07-06 14:16:47,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12\n",
            "[2023-07-06 14:16:47,752] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12\n",
            "[2023-07-06 14:16:47,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11\n",
            "[2023-07-06 14:17:00,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11\n",
            "[2023-07-06 14:17:01,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10\n",
            "[2023-07-06 14:17:01,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10\n",
            "[2023-07-06 14:17:01,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8\n",
            "[2023-07-06 14:17:08,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8\n",
            "[2023-07-06 14:17:09,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7\n",
            "[2023-07-06 14:17:09,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[2023-07-06 14:17:51,491][Main][INFO] - [train] Step 100 out of 65536 | Loss --> 59.114 | Grad_l2 --> inf | Weights_l2 --> 7042.655 | Lr --> 0.010 | Seconds_per_step --> 2.393 | \n",
            "Process ForkProcess-9:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Process ForkProcess-10:\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Open-Transformer/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m72\u001b[0m in \u001b[92m<module>\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m69 \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m70 \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m71 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m72 \u001b[2m│   \u001b[0mmain()                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m73 \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m94\u001b[0m in \u001b[92mdecorated_main\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 92 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# no return value from run_hydra() as it may somet\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# multiple times (--multirun)\u001b[0m                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 94 \u001b[2m│   │   │   │   │   \u001b[0m_run_hydra(                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0margs=args,                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0margs_parser=args_parser,                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtask_function=task_function,                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m394\u001b[0m in      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_run_hydra\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m391 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m392 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m args.run \u001b[95mor\u001b[0m args.multirun:                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m393 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_mode = hydra.get_mode(config_name=config_name, overrid \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m394 \u001b[2m│   │   │   \u001b[0m_run_app(                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m395 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mrun=args.run,                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m396 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmultirun=args.multirun,                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m397 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmode=run_mode,                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m457\u001b[0m in      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_run_app\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m454 \u001b[0m\u001b[2m│   │   │   \u001b[0moverrides.extend([\u001b[33m\"\u001b[0m\u001b[33mhydra.mode=MULTIRUN\u001b[0m\u001b[33m\"\u001b[0m])                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m455 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m mode == RunMode.RUN:                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m457 \u001b[2m│   │   \u001b[0mrun_and_report(                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m458 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mlambda\u001b[0m: hydra.run(                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m459 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mconfig_name=config_name,                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m460 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtask_function=task_function,                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m220\u001b[0m in      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mrun_and_report\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m217 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m218 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mrun_and_report\u001b[0m(func: Any) -> Any:                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m220 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m func()                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m ex:                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m _is_env_set(\u001b[33m\"\u001b[0m\u001b[33mHYDRA_FULL_ERROR\u001b[0m\u001b[33m\"\u001b[0m) \u001b[95mor\u001b[0m is_under_debugger():     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m223 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m ex                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m458\u001b[0m in      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m<lambda>\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m455 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m mode == RunMode.RUN:                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m457 \u001b[0m\u001b[2m│   │   \u001b[0mrun_and_report(                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m458 \u001b[2m│   │   │   \u001b[0m\u001b[94mlambda\u001b[0m: hydra.run(                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m459 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mconfig_name=config_name,                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m460 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtask_function=task_function,                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m461 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moverrides=overrides,                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mhydra.py\u001b[0m:\u001b[94m119\u001b[0m in \u001b[92mrun\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   \u001b[0mcallbacks = Callbacks(cfg)                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0mcallbacks.on_run_start(config=cfg, config_name=config_name)    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m119 \u001b[2m│   │   \u001b[0mret = run_job(                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   │   \u001b[0mhydra_context=HydraContext(                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mconfig_loader=\u001b[96mself\u001b[0m.config_loader, callbacks=callbacks  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   \u001b[0m),                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/core/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m186\u001b[0m in \u001b[92mrun_job\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m env_override(hydra_cfg.hydra.job.env_set):                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   │   │   \u001b[0mcallbacks.on_job_start(config=config, task_function=task_f \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m186 \u001b[2m│   │   │   │   \u001b[0mret.return_value = task_function(task_cfg)             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mret.status = JobStatus.COMPLETED                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mret.return_value = e                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Open-Transformer/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m65\u001b[0m in \u001b[92mmain\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m62 \u001b[0m\u001b[2m│   │   │   \u001b[0mpredict(model, test_dataloader, logger,                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m63 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs, tokenizer)                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m64 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m65 \u001b[2m│   │   \u001b[0mtrain(model, train_dataloader, test_dataloader, accelerator,    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m66 \u001b[0m\u001b[2m│   │   │     \u001b[0mlr_scheduler, optimizer, logger, args, tokenizer)         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m67 \u001b[0m\u001b[2m│   \u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m68 \u001b[0m\u001b[2m│   \u001b[0mlogger.finish()                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Open-Transformer/utils/\u001b[0m\u001b[1;33mtrain_utils.py\u001b[0m:\u001b[94m190\u001b[0m in \u001b[92mtrain\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m args.current_train_step > args.optim.total_steps:       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m190 \u001b[2m│   │   │   \u001b[0mloss, stats = forward(model, batch)                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m191 \u001b[0m\u001b[2m│   │   │   \u001b[0maccelerator.backward(loss / args.optim.grad_acc)           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m192 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrain_averager.update(stats)                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m193 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Open-Transformer/utils/\u001b[0m\u001b[1;33mtrain_utils.py\u001b[0m:\u001b[94m92\u001b[0m in \u001b[92mforward\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   \u001b[0mloss = outputs.loss                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   \u001b[0mstats = {}                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 92 \u001b[2m│   \u001b[0mstats[\u001b[33m'\u001b[0m\u001b[33mloss\u001b[0m\u001b[33m'\u001b[0m] = loss.detach().float().item()                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m calc_acc:                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   \u001b[0mcorrect = (outputs.logits.argmax(-\u001b[94m1\u001b[0m) == batch[\u001b[33m\"\u001b[0m\u001b[33mlabels\u001b[0m\u001b[33m\"\u001b[0m]).sum() \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mKeyboardInterrupt\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetuning"
      ],
      "metadata": {
        "id": "QGYoY8hHc2je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Open-Transformer\n",
        "!git clone https://github.com/allenai/natural-instructions.git\n",
        "!cp natural-instructions/* data/"
      ],
      "metadata": {
        "id": "XchhJ-JlPYIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Open-Transformer\n",
        "!python main.py task=ft model.name=google/t5-v1_1-base model.random_init=false model.checkpoint_path=\"\" model.compile=true optim.batch_size=4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1zV990mQmuX",
        "outputId": "aa671ea6-1688-4145-a21b-7d35aa4fbc74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Open-Transformer\n",
            "2023-07-07 12:58:35.966448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-07-07 12:58:42,015][Main][INFO] - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "[2023-07-07 12:58:42,016][Main][INFO] - Working directory is /content/drive/MyDrive/Open-Transformer/logs/2023-07-07/12-58-41-\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.30.2\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/t5-v1_1-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.30.2\"\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[2023-07-07 12:58:49,213][datasets.builder][WARNING] - Found cached dataset ni_dataset (/root/.cache/huggingface/datasets/ni_dataset/default-fcc6392edb2404b3/2.0.0/0e355e8fd2262ee2af826ecea71aa0328eb6a446a20e845a2dee1d65517c6977)\n",
            "100% 2/2 [00:00<00:00, 522.07it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[2023-07-07 12:58:53,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-07 12:59:01,918] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
            "[2023-07-07 12:59:02,098] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-07 12:59:27,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0\n",
            "[2023-07-07 12:59:43,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0\n",
            "[2023-07-07 12:59:43,651] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-07 12:59:56,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__\n",
            "[2023-07-07 12:59:56,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index\n",
            "[2023-07-07 12:59:56,080] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__\n",
            "[2023-07-07 12:59:56,191] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0\n",
            "[2023-07-07 13:00:15,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0\n",
            "[2023-07-07 13:00:32,630] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-07 13:00:39,562] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
            "[2023-07-07 13:00:39,733] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-07 13:01:02,761] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1\n",
            "[2023-07-07 13:01:16,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1\n",
            "[2023-07-07 13:01:16,383] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-07 13:01:37,215] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1\n",
            "[2023-07-07 13:01:56,163] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1\n",
            "[2023-07-07 13:02:12,626] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-07 13:02:20,427] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
            "[2023-07-07 13:02:20,600] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-07 13:02:42,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2\n",
            "[2023-07-07 13:02:57,430] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2\n",
            "[2023-07-07 13:02:57,433] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-07 13:03:15,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2\n",
            "[2023-07-07 13:03:34,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2\n",
            "[2023-07-07 13:03:52,101] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-07 13:03:58,318] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
            "[2023-07-07 13:03:58,489] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-07-07 13:04:22,333] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3\n",
            "[2023-07-07 13:04:33,146] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3\n",
            "[2023-07-07 13:04:33,149] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-07-07 13:04:35,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3\n",
            "[2023-07-07 13:04:50,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3\n",
            "[2023-07-07 13:04:52,810] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-07-07 13:05:01,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
            "[2023-07-07 13:05:01,839] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "Process ForkProcess-10:\n",
            "Process ForkProcess-9:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f945fba72e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt: \n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Open-Transformer/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m72\u001b[0m in \u001b[92m<module>\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m69 \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m70 \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m71 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m72 \u001b[2m│   \u001b[0mmain()                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m73 \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m94\u001b[0m in \u001b[92mdecorated_main\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 92 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# no return value from run_hydra() as it may somet\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# multiple times (--multirun)\u001b[0m                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 94 \u001b[2m│   │   │   │   │   \u001b[0m_run_hydra(                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0margs=args,                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0margs_parser=args_parser,                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtask_function=task_function,                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m394\u001b[0m in      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_run_hydra\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m391 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m392 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m args.run \u001b[95mor\u001b[0m args.multirun:                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m393 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_mode = hydra.get_mode(config_name=config_name, overrid \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m394 \u001b[2m│   │   │   \u001b[0m_run_app(                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m395 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mrun=args.run,                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m396 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmultirun=args.multirun,                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m397 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmode=run_mode,                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m457\u001b[0m in      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_run_app\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m454 \u001b[0m\u001b[2m│   │   │   \u001b[0moverrides.extend([\u001b[33m\"\u001b[0m\u001b[33mhydra.mode=MULTIRUN\u001b[0m\u001b[33m\"\u001b[0m])                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m455 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m mode == RunMode.RUN:                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m457 \u001b[2m│   │   \u001b[0mrun_and_report(                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m458 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mlambda\u001b[0m: hydra.run(                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m459 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mconfig_name=config_name,                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m460 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtask_function=task_function,                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m220\u001b[0m in      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mrun_and_report\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m217 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m218 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mrun_and_report\u001b[0m(func: Any) -> Any:                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m220 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m func()                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m ex:                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m _is_env_set(\u001b[33m\"\u001b[0m\u001b[33mHYDRA_FULL_ERROR\u001b[0m\u001b[33m\"\u001b[0m) \u001b[95mor\u001b[0m is_under_debugger():     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m223 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m ex                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m458\u001b[0m in      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m<lambda>\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m455 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m mode == RunMode.RUN:                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m457 \u001b[0m\u001b[2m│   │   \u001b[0mrun_and_report(                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m458 \u001b[2m│   │   │   \u001b[0m\u001b[94mlambda\u001b[0m: hydra.run(                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m459 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mconfig_name=config_name,                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m460 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtask_function=task_function,                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m461 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moverrides=overrides,                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/_internal/\u001b[0m\u001b[1;33mhydra.py\u001b[0m:\u001b[94m119\u001b[0m in \u001b[92mrun\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   \u001b[0mcallbacks = Callbacks(cfg)                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0mcallbacks.on_run_start(config=cfg, config_name=config_name)    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m119 \u001b[2m│   │   \u001b[0mret = run_job(                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   │   \u001b[0mhydra_context=HydraContext(                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mconfig_loader=\u001b[96mself\u001b[0m.config_loader, callbacks=callbacks  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   \u001b[0m),                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/hydra/core/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m186\u001b[0m in \u001b[92mrun_job\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m env_override(hydra_cfg.hydra.job.env_set):                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   │   │   \u001b[0mcallbacks.on_job_start(config=config, task_function=task_f \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m186 \u001b[2m│   │   │   │   \u001b[0mret.return_value = task_function(task_cfg)             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mret.status = JobStatus.COMPLETED                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mret.return_value = e                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Open-Transformer/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m65\u001b[0m in \u001b[92mmain\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m62 \u001b[0m\u001b[2m│   │   │   \u001b[0mpredict(model, test_dataloader, logger,                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m63 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs, tokenizer)                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m64 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m65 \u001b[2m│   │   \u001b[0mtrain(model, train_dataloader, test_dataloader, accelerator,    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m66 \u001b[0m\u001b[2m│   │   │     \u001b[0mlr_scheduler, optimizer, logger, args, tokenizer)         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m67 \u001b[0m\u001b[2m│   \u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m68 \u001b[0m\u001b[2m│   \u001b[0mlogger.finish()                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Open-Transformer/utils/\u001b[0m\u001b[1;33mtrain_utils.py\u001b[0m:\u001b[94m190\u001b[0m in \u001b[92mtrain\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m args.current_train_step > args.optim.total_steps:       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m190 \u001b[2m│   │   │   \u001b[0mloss, stats = forward(model, batch)                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m191 \u001b[0m\u001b[2m│   │   │   \u001b[0maccelerator.backward(loss / args.optim.grad_acc)           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m192 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrain_averager.update(stats)                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m193 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Open-Transformer/utils/\u001b[0m\u001b[1;33mtrain_utils.py\u001b[0m:\u001b[94m88\u001b[0m in \u001b[92mforward\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 85 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 86 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 87 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(model, batch, calc_acc=\u001b[94mFalse\u001b[0m):                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 88 \u001b[2m│   \u001b[0moutputs = model(**batch)                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   \u001b[0mloss = outputs.loss                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   \u001b[0mstats = {}                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hoo \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33meval_frame.py\u001b[0m:\u001b[94m82\u001b[0m in    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mgetattr\u001b[0m(\u001b[96mself\u001b[0m._orig_mod, name)                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 80 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 82 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.dynamo_ctx(\u001b[96mself\u001b[0m._orig_mod.forward)(*args, **kwargs \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 83 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 84 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 85 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mremove_from_cache\u001b[0m(f):                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33meval_frame.py\u001b[0m:\u001b[94m209\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_fn\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m206 \u001b[0m\u001b[2m│   │   │   \u001b[0mdynamic_ctx = enable_dynamic(\u001b[96mself\u001b[0m.dynamic)                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m207 \u001b[0m\u001b[2m│   │   │   \u001b[0mdynamic_ctx.\u001b[92m__enter__\u001b[0m()                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m208 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m209 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m fn(*args, **kwargs)                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m210 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m211 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mset_eval_frame(prior)                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m212 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdynamic_ctx.\u001b[92m__exit__\u001b[0m(\u001b[94mNone\u001b[0m, \u001b[94mNone\u001b[0m, \u001b[94mNone\u001b[0m)                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33meval_frame.py\u001b[0m:\u001b[94m337\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mcatch_errors\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m334 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m hijacked_callback(frame, cache_size, hooks) \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m335 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m336 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m compile_lock:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m337 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m callback(frame, cache_size, hooks)                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m338 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m339 \u001b[0m\u001b[2m│   \u001b[0mcatch_errors._torchdynamo_orig_callable = callback  \u001b[2m# type: ignore\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m340 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m catch_errors                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mconvert_frame.py\u001b[0m:\u001b[94m404\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m_convert_frame\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m401 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_convert_frame\u001b[0m(frame: types.FrameType, cache_size: \u001b[96mint\u001b[0m, hooks: \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m402 \u001b[0m\u001b[2m│   │   \u001b[0mcounters[\u001b[33m\"\u001b[0m\u001b[33mframes\u001b[0m\u001b[33m\"\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mtotal\u001b[0m\u001b[33m\"\u001b[0m] += \u001b[94m1\u001b[0m                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m403 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m404 \u001b[2m│   │   │   \u001b[0mresult = inner_convert(frame, cache_size, hooks)           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m405 \u001b[0m\u001b[2m│   │   │   \u001b[0mcounters[\u001b[33m\"\u001b[0m\u001b[33mframes\u001b[0m\u001b[33m\"\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mok\u001b[0m\u001b[33m\"\u001b[0m] += \u001b[94m1\u001b[0m                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m406 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m result                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m407 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m (\u001b[96mNotImplementedError\u001b[0m, Unsupported):                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mconvert_frame.py\u001b[0m:\u001b[94m104\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m_fn\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   │   \u001b[0mprior_fwd_from_src = torch.fx.graph_module._forward_from_src   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m102 \u001b[0m\u001b[2m│   │   \u001b[0mtorch.fx.graph_module._forward_from_src = fx_forward_from_src_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m104 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m fn(*args, **kwargs)                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m│   │   │   \u001b[0mtorch._C._set_grad_enabled(prior_grad_mode)                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   │   \u001b[0mtorch.random.set_rng_state(rng_state)                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mconvert_frame.py\u001b[0m:\u001b[94m262\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m_convert_frame_assert\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m259 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mglobal\u001b[0m initial_grad_state                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m\u001b[2m│   │   \u001b[0minitial_grad_state = torch.is_grad_enabled()                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m262 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _compile(                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m263 \u001b[0m\u001b[2m│   │   │   \u001b[0mframe.f_code,                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m264 \u001b[0m\u001b[2m│   │   │   \u001b[0mframe.f_globals,                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m265 \u001b[0m\u001b[2m│   │   │   \u001b[0mframe.f_locals,                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m163\u001b[0m in        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mtime_wrapper\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 160 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m key \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m compilation_metrics:                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 161 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcompilation_metrics[key] = []                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 162 \u001b[0m\u001b[2m│   │   │   \u001b[0mt0 = time.time()                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 163 \u001b[2m│   │   │   \u001b[0mr = func(*args, **kwargs)                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 164 \u001b[0m\u001b[2m│   │   │   \u001b[0mtime_spent = time.time() - t0                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 165 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} \u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 166 \u001b[0m\u001b[2m│   │   │   \u001b[0mcompilation_metrics[key].append(time_spent)               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mconvert_frame.py\u001b[0m:\u001b[94m324\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m_compile\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m321 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m322 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m attempt \u001b[95min\u001b[0m itertools.count():                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m323 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m324 \u001b[2m│   │   │   │   \u001b[0mout_code = transform_code_object(code, transform)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m325 \u001b[0m\u001b[2m│   │   │   │   \u001b[0morig_code_map[out_code] = code                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m326 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m327 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m exc.RestartAnalysis:                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mbytecode_transformatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33mn.py\u001b[0m:\u001b[94m445\u001b[0m in \u001b[92mtransform_code_object\u001b[0m                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m442 \u001b[0m\u001b[2m│   \u001b[0minstructions = cleaned_instructions(code, safe)                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m443 \u001b[0m\u001b[2m│   \u001b[0mpropagate_line_nums(instructions)                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m444 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m445 \u001b[2m│   \u001b[0mtransformations(instructions, code_options)                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m446 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m clean_and_assemble_instructions(instructions, keys, code_op \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m447 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m448 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mconvert_frame.py\u001b[0m:\u001b[94m311\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mtransform\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m308 \u001b[0m\u001b[2m│   │   │   \u001b[0mexport,                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m309 \u001b[0m\u001b[2m│   │   │   \u001b[0mmutated_closure_cell_contents,                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m310 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m311 \u001b[2m│   │   \u001b[0mtracer.run()                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m312 \u001b[0m\u001b[2m│   │   \u001b[0moutput = tracer.output                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m output \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m output.output_instructions                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33msymbolic_convert.py\u001b[0m:\u001b[94m17\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m26\u001b[0m in \u001b[92mrun\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1723 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1724 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mrun\u001b[0m(\u001b[96mself\u001b[0m):                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1725 \u001b[0m\u001b[2m│   │   \u001b[0m_step_logger()(logging.INFO, \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mtorchdynamo start tracing \u001b[0m\u001b[33m{\u001b[0m\u001b[96msel\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1726 \u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().run()                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1727 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1728 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mmatch_nested_cell\u001b[0m(\u001b[96mself\u001b[0m, name, cell):                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1729 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Match a cell in this method to one in a function we are in\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33msymbolic_convert.py\u001b[0m:\u001b[94m57\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m6\u001b[0m in \u001b[92mrun\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 573 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwhile\u001b[0m (                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 574 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.instruction_pointer \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 575 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m.output.should_exit                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 576 \u001b[2m│   │   │   │   \u001b[0m\u001b[95mand\u001b[0m \u001b[96mself\u001b[0m.step()                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 577 \u001b[0m\u001b[2m│   │   │   \u001b[0m):                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 578 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mpass\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 579 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m BackendCompilerFailed:                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33msymbolic_convert.py\u001b[0m:\u001b[94m54\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m0\u001b[0m in \u001b[92mstep\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 537 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 538 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mhasattr\u001b[0m(\u001b[96mself\u001b[0m, inst.opname):                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 539 \u001b[0m\u001b[2m│   │   │   │   \u001b[0munimplemented(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mmissing: \u001b[0m\u001b[33m{\u001b[0minst.opname\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 540 \u001b[2m│   │   │   \u001b[0m\u001b[96mgetattr\u001b[0m(\u001b[96mself\u001b[0m, inst.opname)(inst)                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 541 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 542 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m inst.opname != \u001b[33m\"\u001b[0m\u001b[33mRETURN_VALUE\u001b[0m\u001b[33m\"\u001b[0m                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 543 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m BackendCompilerFailed:                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33msymbolic_convert.py\u001b[0m:\u001b[94m17\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m92\u001b[0m in \u001b[92mRETURN_VALUE\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1789 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mtorchdynamo done tracing \u001b[0m\u001b[33m{\u001b[0m\u001b[96mself\u001b[0m.f_code.co_name\u001b[33m}\u001b[0m\u001b[33m (RETURN_\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1790 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1791 \u001b[0m\u001b[2m│   │   \u001b[0mlog.debug(\u001b[33m\"\u001b[0m\u001b[33mRETURN_VALUE triggered compile\u001b[0m\u001b[33m\"\u001b[0m)                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1792 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.output.compile_subgraph(                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1793 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, reason=GraphCompileReason(\u001b[33m\"\u001b[0m\u001b[33mreturn_value\u001b[0m\u001b[33m\"\u001b[0m, [\u001b[96mself\u001b[0m.fra \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1794 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1795 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.output.add_output_instructions([create_instruction(\u001b[33m\"\u001b[0m\u001b[33mRETU\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33moutput_graph.py\u001b[0m:\u001b[94m541\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mcompile_subgraph\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m538 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput = []                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m539 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m count_calls(\u001b[96mself\u001b[0m.graph) != \u001b[94m0\u001b[0m \u001b[95mor\u001b[0m \u001b[96mlen\u001b[0m(pass2.graph_outputs \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m540 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput.extend(                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m541 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.compile_and_call_fx_graph(tx, pass2.graph_out \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m542 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m543 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m544 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(pass2.graph_outputs) != \u001b[94m0\u001b[0m:                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33moutput_graph.py\u001b[0m:\u001b[94m588\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mcompile_and_call_fx_graph\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m585 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m586 \u001b[0m\u001b[2m│   │   \u001b[0massert_no_fake_params_or_buffers(gm)                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m587 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m tracing(\u001b[96mself\u001b[0m.tracing_context):                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m588 \u001b[2m│   │   │   \u001b[0mcompiled_fn = \u001b[96mself\u001b[0m.call_user_compiler(gm)                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m589 \u001b[0m\u001b[2m│   │   \u001b[0mcompiled_fn = disable(compiled_fn)                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m590 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m591 \u001b[0m\u001b[2m│   │   \u001b[0mcounters[\u001b[33m\"\u001b[0m\u001b[33mstats\u001b[0m\u001b[33m\"\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33munique_graphs\u001b[0m\u001b[33m\"\u001b[0m] += \u001b[94m1\u001b[0m                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m163\u001b[0m in        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mtime_wrapper\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 160 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m key \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m compilation_metrics:                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 161 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcompilation_metrics[key] = []                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 162 \u001b[0m\u001b[2m│   │   │   \u001b[0mt0 = time.time()                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 163 \u001b[2m│   │   │   \u001b[0mr = func(*args, **kwargs)                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 164 \u001b[0m\u001b[2m│   │   │   \u001b[0mtime_spent = time.time() - t0                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 165 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} \u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 166 \u001b[0m\u001b[2m│   │   │   \u001b[0mcompilation_metrics[key].append(time_spent)               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33moutput_graph.py\u001b[0m:\u001b[94m670\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mcall_user_compiler\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m667 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m config.DO_NOT_USE_legacy_non_fake_example_inputs:     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m668 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcompiled_fn = compiler_fn(gm, \u001b[96mself\u001b[0m.example_inputs())   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m669 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m670 \u001b[2m│   │   │   │   \u001b[0mcompiled_fn = compiler_fn(gm, \u001b[96mself\u001b[0m.fake_example_inputs \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m671 \u001b[0m\u001b[2m│   │   │   \u001b[0m_step_logger()(logging.INFO, \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mdone compiler function \u001b[0m\u001b[33m{\u001b[0mnam \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m672 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94massert\u001b[0m callable(compiled_fn), \u001b[33m\"\u001b[0m\u001b[33mcompiler_fn did not return \u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m673 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mdebug_utils.py\u001b[0m:\u001b[94m1055\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mdebug_wrapper\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1052 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m)                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1053 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1055 \u001b[2m│   │   │   \u001b[0mcompiled_gm = compiler_fn(gm, example_inputs)             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1056 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1057 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m compiled_gm                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1058 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m1390\u001b[0m in \u001b[92m__call__\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1387 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__call__\u001b[0m(\u001b[96mself\u001b[0m, model_, inputs_):                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1388 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96m_inductor\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mcompile_fx\u001b[0m \u001b[94mimport\u001b[0m compile_fx             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1389 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1390 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m compile_fx(model_, inputs_, config_patches=\u001b[96mself\u001b[0m.config \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1391 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1392 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1393 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcompile\u001b[0m(model: Optional[Callable] = \u001b[94mNone\u001b[0m, *,                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_inductor/\u001b[0m\u001b[1;33mcompile_fx.py\u001b[0m:\u001b[94m455\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mcompile_fx\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m452 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# TODO: can add logging before/after the call to create_aot_di\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m453 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# in torch._functorch/aot_autograd.py::aot_module_simplified::\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m454 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# once torchdynamo is merged into pytorch\u001b[0m                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m455 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m aot_autograd(                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   │   │   \u001b[0mfw_compiler=fw_compiler,                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m457 \u001b[0m\u001b[2m│   │   │   \u001b[0mbw_compiler=bw_compiler,                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m458 \u001b[0m\u001b[2m│   │   │   \u001b[0mdecompositions=select_decomp_table(),                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/\u001b[0m\u001b[1;33mcommon.py\u001b[0m:\u001b[94m48\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mcompiler_fn\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 45 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 46 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# NB: NOT cloned!\u001b[0m                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 47 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m enable_aot_logging():                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 48 \u001b[2m│   │   │   │   \u001b[0mcg = aot_module_simplified(gm, example_inputs, **kwarg \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 49 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcounters[\u001b[33m\"\u001b[0m\u001b[33maot_autograd\u001b[0m\u001b[33m\"\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mok\u001b[0m\u001b[33m\"\u001b[0m] += \u001b[94m1\u001b[0m                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 50 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m eval_frame.disable(cg)                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 51 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m:                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_functorch/\u001b[0m\u001b[1;33maot_autograd.py\u001b[0m:\u001b[94m282\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m2\u001b[0m in \u001b[92maot_module_simplified\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2819 \u001b[0m\u001b[2m│   \u001b[0mfull_args.extend(params_flat)                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2820 \u001b[0m\u001b[2m│   \u001b[0mfull_args.extend(args)                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2821 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2822 \u001b[2m│   \u001b[0mcompiled_fn = create_aot_dispatcher_function(                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2823 \u001b[0m\u001b[2m│   │   \u001b[0mfunctional_call,                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2824 \u001b[0m\u001b[2m│   │   \u001b[0mfull_args,                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2825 \u001b[0m\u001b[2m│   │   \u001b[0maot_config,                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m163\u001b[0m in        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mtime_wrapper\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 160 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m key \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m compilation_metrics:                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 161 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcompilation_metrics[key] = []                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 162 \u001b[0m\u001b[2m│   │   │   \u001b[0mt0 = time.time()                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 163 \u001b[2m│   │   │   \u001b[0mr = func(*args, **kwargs)                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 164 \u001b[0m\u001b[2m│   │   │   \u001b[0mtime_spent = time.time() - t0                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 165 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} \u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 166 \u001b[0m\u001b[2m│   │   │   \u001b[0mcompilation_metrics[key].append(time_spent)               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_functorch/\u001b[0m\u001b[1;33maot_autograd.py\u001b[0m:\u001b[94m251\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m5\u001b[0m in \u001b[92mcreate_aot_dispatcher_function\u001b[0m                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2512 \u001b[0m\u001b[2m│   │   \u001b[0mcompiler_fn = partial(aot_wrapper_dedupe, compiler_fn=compile \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2513 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# You can put more passes here\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2514 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2515 \u001b[2m│   │   \u001b[0mcompiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2516 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2517 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mhasattr\u001b[0m(compiled_fn, \u001b[33m\"\u001b[0m\u001b[33m_boxed_call\u001b[0m\u001b[33m\"\u001b[0m):                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2518 \u001b[0m\u001b[2m│   │   │   \u001b[0mcompiled_fn = make_boxed_func(compiled_fn)                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_functorch/\u001b[0m\u001b[1;33maot_autograd.py\u001b[0m:\u001b[94m171\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m5\u001b[0m in \u001b[92maot_wrapper_dedupe\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1712 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1713 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1714 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m ok:                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1715 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m compiler_fn(flat_fn, leaf_flat_args, aot_config)   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1716 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1717 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Strategy 2: Duplicate specialize.\u001b[0m                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1718 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m#\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_functorch/\u001b[0m\u001b[1;33maot_autograd.py\u001b[0m:\u001b[94m213\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m1\u001b[0m in \u001b[92maot_dispatch_autograd\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2128 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2129 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m track_graph_compiling(aot_config, \u001b[33m\"\u001b[0m\u001b[33mjoint\u001b[0m\u001b[33m\"\u001b[0m):              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2130 \u001b[0m\u001b[2m│   │   │   \u001b[0mnum_inner_fwd_outputs = metadata_.num_mutated_inputs + me \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2131 \u001b[2m│   │   │   \u001b[0mfw_module, bw_module = aot_config.partition_fn(           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2132 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mfx_g, joint_inputs, num_fwd_outputs=num_inner_fwd_out \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2133 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2134 \u001b[0m\u001b[2m│   │   │   \u001b[0mfw_outs = [n \u001b[94mfor\u001b[0m n \u001b[95min\u001b[0m fw_module.graph.nodes \u001b[94mif\u001b[0m n.op == \u001b[33m\"\u001b[0m\u001b[33mo\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_functorch/\u001b[0m\u001b[1;33mpartitioners.py\u001b[0m:\u001b[94m502\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mmin_cut_rematerialization_partition\u001b[0m                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m499 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m user \u001b[95min\u001b[0m node.users:                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m500 \u001b[0m\u001b[2m│   │   │   \u001b[0mnx_graph.add_edge(node.name + \u001b[33m\"\u001b[0m\u001b[33m_out\u001b[0m\u001b[33m\"\u001b[0m, user.name + \u001b[33m\"\u001b[0m\u001b[33m_in\u001b[0m\u001b[33m\"\u001b[0m, c \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m501 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m502 \u001b[2m│   \u001b[0mcut_value, partition = nx.minimum_cut(nx_graph, \u001b[33m\"\u001b[0m\u001b[33msource\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33msink\u001b[0m\u001b[33m\"\u001b[0m)  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m503 \u001b[0m\u001b[2m│   \u001b[0mreachable, non_reachable = partition                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m504 \u001b[0m\u001b[2m│   \u001b[0mcutset = \u001b[96mset\u001b[0m()                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m505 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m u, nbrs \u001b[95min\u001b[0m ((n, nx_graph[n]) \u001b[94mfor\u001b[0m n \u001b[95min\u001b[0m reachable):              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/networkx/algorithms/flow/\u001b[0m\u001b[1;33mmaxflow.py\u001b[0m: \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m450\u001b[0m in \u001b[92mminimum_cut\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m447 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m kwargs.get(\u001b[33m\"\u001b[0m\u001b[33mcutoff\u001b[0m\u001b[33m\"\u001b[0m) \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m flow_func \u001b[95mis\u001b[0m preflow_push: \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m448 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m nx.NetworkXError(\u001b[33m\"\u001b[0m\u001b[33mcutoff should not be specified.\u001b[0m\u001b[33m\"\u001b[0m)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m449 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m450 \u001b[2m│   \u001b[0mR = flow_func(flowG, _s, _t, capacity=capacity, value_only=\u001b[94mTrue\u001b[0m, * \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m451 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Remove saturated edges from the residual network\u001b[0m                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m452 \u001b[0m\u001b[2m│   \u001b[0mcutset = [(u, v, d) \u001b[94mfor\u001b[0m u, v, d \u001b[95min\u001b[0m R.edges(data=\u001b[94mTrue\u001b[0m) \u001b[94mif\u001b[0m d[\u001b[33m\"\u001b[0m\u001b[33mflow\u001b[0m\u001b[33m\"\u001b[0m] \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m453 \u001b[0m\u001b[2m│   \u001b[0mR.remove_edges_from(cutset)                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/networkx/algorithms/flow/\u001b[0m\u001b[1;33mpreflowpush\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m421\u001b[0m in \u001b[92mpreflow_push\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m418 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mTrue\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m419 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m420 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m421 \u001b[2m│   \u001b[0mR = preflow_push_impl(G, s, t, capacity, residual, global_relabel_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m422 \u001b[0m\u001b[2m│   \u001b[0mR.graph[\u001b[33m\"\u001b[0m\u001b[33malgorithm\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[33m\"\u001b[0m\u001b[33mpreflow_push\u001b[0m\u001b[33m\"\u001b[0m                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m423 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m R                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m424 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/networkx/algorithms/flow/\u001b[0m\u001b[1;33mpreflowpush\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m50\u001b[0m in \u001b[92mpreflow_push_impl\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 47 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Initialize/reset the residual network.\u001b[0m                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 48 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m u \u001b[95min\u001b[0m R:                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 49 \u001b[0m\u001b[2m│   │   \u001b[0mR_nodes[u][\u001b[33m\"\u001b[0m\u001b[33mexcess\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[94m0\u001b[0m                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 50 \u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m e \u001b[95min\u001b[0m R_succ[u].values():                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 51 \u001b[0m\u001b[2m│   │   │   \u001b[0me[\u001b[33m\"\u001b[0m\u001b[33mflow\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[94m0\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 52 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 53 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mreverse_bfs\u001b[0m(src):                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.10/\u001b[0m\u001b[1;33m_collections_abc.py\u001b[0m:\u001b[94m930\u001b[0m in \u001b[92m__iter__\u001b[0m                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 927 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 928 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__iter__\u001b[0m(\u001b[96mself\u001b[0m):                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 929 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m key \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._mapping:                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 930 \u001b[2m│   │   │   \u001b[0m\u001b[94myield\u001b[0m \u001b[96mself\u001b[0m._mapping[key]                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 931 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 932 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 933 \u001b[0mValuesView.register(dict_values)                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/networkx/classes/\u001b[0m\u001b[1;33mcoreviews.py\u001b[0m:\u001b[94m52\u001b[0m in  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m__getitem__\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 49 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__iter__\u001b[0m(\u001b[96mself\u001b[0m):                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 50 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96miter\u001b[0m(\u001b[96mself\u001b[0m._atlas)                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 51 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 52 \u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__getitem__\u001b[0m(\u001b[96mself\u001b[0m, key):                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 53 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._atlas[key]                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 54 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 55 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcopy\u001b[0m(\u001b[96mself\u001b[0m):                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mKeyboardInterrupt\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}